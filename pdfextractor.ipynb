{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for identifying pages that contain keywords, finding the questions, extracting and translating them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 14, 18, 29, 31, 32, 41, 43, 45, 48, 55, 57, 58, 184, 188, 254, 274, 334, 336, 183, 186, 151, 185, 254, 258, 259, 261, 262, 3, 4, 12, 34, 107, 108, 109, 191, 332, 106, 107, 2, 4, 5, 6, 7, 14, 17, 18, 23, 25, 28, 30, 31, 32, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 48, 52, 55, 56, 57, 58, 63, 64, 69, 70, 71, 72, 73, 74, 77, 83, 84, 85, 86, 87, 88, 90, 91, 97, 98, 99, 105, 117, 124, 125, 126, 127, 128, 130, 131, 132, 134, 153, 157, 164, 165, 170, 173, 177, 179, 180, 185, 189, 194, 196, 197, 198, 199, 200, 201, 203, 206, 212, 218, 227, 228, 232, 241, 251, 252, 254, 255, 257, 258, 259, 260, 261, 262, 263, 270, 271, 272, 273, 274, 275, 279, 281, 283, 287, 288, 290, 291, 292, 294, 297, 298, 300, 302, 303, 307, 309, 311, 314, 321, 322, 323, 325, 326, 327, 330, 331, 337, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 70, 79, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 103, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 134, 135, 136, 137, 140, 151, 152, 153, 154, 156, 158, 159, 161, 182, 183, 193, 259, 262, 274, 299, 305, 322, 339]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The address sheet was created by mistake If RE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Answer in clear)……………………………… Go to VALIDF If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unusual and uncertain until the end of data co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>» if BS &gt; 0 (Y14) According to you, is the acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A leisure or holiday residence?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>If SITIND = (B), (C), (D) or (SITIND = (A) and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>No NSP REFUSAL If EXRMI=1 and if CAISSALAR =1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>Has this benefit been paid?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>To a person outside the current household DK R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>No REFUSAL Did other people contribute to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Translated Questions\n",
       "0    The address sheet was created by mistake If RE...\n",
       "1    (Answer in clear)……………………………… Go to VALIDF If ...\n",
       "2    Unusual and uncertain until the end of data co...\n",
       "3    » if BS > 0 (Y14) According to you, is the acc...\n",
       "4                      A leisure or holiday residence?\n",
       "..                                                 ...\n",
       "731  If SITIND = (B), (C), (D) or (SITIND = (A) and...\n",
       "732  No NSP REFUSAL If EXRMI=1 and if CAISSALAR =1,...\n",
       "733                        Has this benefit been paid?\n",
       "734  To a person outside the current household DK R...\n",
       "735  No REFUSAL Did other people contribute to the ...\n",
       "\n",
       "[736 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import PyPDF2\n",
    "import re\n",
    "import operator \n",
    "import numpy as np\n",
    "\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to test different regex expressions\n",
    "#so findall definitely works for matching aceented characters \n",
    "#problem is with the pdf extractor that we are using\n",
    "def test_regex():\n",
    "    matched = (re.findall('é','hé'))\n",
    "    print(\"matched\",matched)\n",
    "\n",
    "\n",
    "\n",
    "#translator function - given an array of lines, translate each line in the array, add to array of translated lines,\n",
    "#and return \n",
    "def translator(lines):\n",
    "    translated_array  = []\n",
    "    for i in lines:\n",
    "        to_translate = i \n",
    "        translated  = GoogleTranslator(source='auto',target='en').translate(i)\n",
    "        translated_array.append(translated)\n",
    "    return translated_array \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cleans text from any whitesace and can later be used to remove punctuation if necessary \n",
    "def clean(text):\n",
    "    text = re.sub('\\n','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #removing punctuation \n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text \n",
    "\n",
    "\n",
    "#contains the kwyrods for each poverty question and translates them into the target language - change later so that \n",
    "#you can change the language we are using \n",
    "def keywords():\n",
    "    questions_to_keywords={\n",
    "    \"holiday\":\"Can your whole household afford to go for a week’s annual holiday, away from home?\",\n",
    "    \"vegetarian\":\"Can your household afford a meal with meat, chicken, fish(or vegetarian equivalent)?\",\n",
    "    \"expense\": \"Can your household afford an unexpected required expense(amount to be filled) and pay through its own resources?\",\n",
    "    \"telephone\":\"Does your household have a telephone(fixed landline or mobile)?\",\n",
    "    \"colour TV\":\"Does your household have a color TV?\",\n",
    "    \"washing machine\":\"Does the household have a washing machine? \",\n",
    "    \"van\":\"Does your household have a car/van for private use? \",\n",
    "    \"dwelling\":\"Do you have any of the following problems with your dwelling/accommodation? \",\n",
    "    \"warm\":\"Can your household afford to keep its home adequately warm?  \"\n",
    "    } \n",
    "    translated_keywords_dict = defaultdict()\n",
    "    for key in questions_to_keywords.keys():\n",
    "        translated_keywords_dict[GoogleTranslator(source='en', target='french').translate(key)] = []\n",
    "    return translated_keywords_dict\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#iterates through an array which contains page numbers, extracts each quesiton from that page, translates them into english,\n",
    "#adds to an array, cleans data and adds to final array \n",
    "def translate_document(pages):\n",
    "    pdf1 = pdfplumber.open(\"france.pdf\")\n",
    "    translated_array = []\n",
    "    #pages = list(pages)\n",
    "    # writing page 161 will translate page 162\n",
    "    pages= pages\n",
    "    for number in pages:\n",
    "        p1 = pdf1.pages[number]\n",
    "        im = p1.to_image()\n",
    "        text = p1.extract_text()\n",
    "        text = clean(text)\n",
    "        #text = re.split('[?]',text)  \n",
    "        text = re.findall('(?<=[\\?\\.\\!]\\s)[^\\?\\n\\.]+?\\?',text)\n",
    "        clean_sent  = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent) \n",
    "        #translated_array.append(clean_sent)\n",
    "        translated_array.append(translator(clean_sent))\n",
    "    return translated_array\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    translated_keywords = keywords().keys()\n",
    "    pages = []\n",
    "    pdf = pdfplumber.open(\"france.pdf\")\n",
    "    for word in translated_keywords:\n",
    "        word = word.lower()\n",
    "        for i in range(0,len(pdf.pages)):\n",
    "            page_number = pdf.pages[i]\n",
    "            Text = page_number.extract_text()\n",
    "            if re.findall(word,Text,re.IGNORECASE):\n",
    "                pages.append(i)\n",
    "    print(pages)\n",
    "    clean_translations = flatten_list(translate_document(pages))\n",
    "    #d = {'Translated Questions':translate_document(pages)}\n",
    "    d = {'Translated Questions':clean_translations}\n",
    "    DftranslatedDoc=pd.DataFrame(data =d)\n",
    "    display(DftranslatedDoc) \n",
    "    DftranslatedDoc.to_csv('out_translation.csv',index=False) \n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Beginning of NLP Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import PyPDF2\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "pdf = pdfplumber.open(\"france.pdf\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words= stopwords.words('english') # can be changed to ('french')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to test different regex expressions\n",
    "#so findall definitely works for matching aceented characters \n",
    "#problem is with the pdf extractor that we are using\n",
    "def test_regex():\n",
    "    matched = (re.findall('é','hé'))\n",
    "    print(\"matched\",matched)\n",
    "\n",
    "\n",
    "\n",
    "#translator function - given an array of lines, translate each line in the array, add to array of translated lines,\n",
    "#and return \n",
    "def translator(lines):\n",
    "    translated_array  = []\n",
    "    for i in lines:\n",
    "        to_translate = i \n",
    "        translated  = GoogleTranslator(source='auto',target='en').translate(i)\n",
    "        translated_array.append(translated)\n",
    "    return translated_array \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cleans text from any whitesace and can later be used to remove punctuation if necessary \n",
    "def clean(text):\n",
    "    text = re.sub('\\n','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #removing punctuation \n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text \n",
    "\n",
    "\n",
    "#contains the kwyrods for each poverty question and translates them into the target language - change later so that \n",
    "#you can change the language we are using \n",
    "def keywords():\n",
    "    questions_to_keywords={\n",
    "    \"holiday\":\"Can your whole household afford to go for a week’s annual holiday, away from home?\",\n",
    "    \"vegetarian\":\"Can your household afford a meal with meat, chicken, fish(or vegetarian equivalent)?\",\n",
    "    \"expense\": \"Can your household afford an unexpected required expense(amount to be filled) and pay through its own resources?\",\n",
    "    \"telephone\":\"Does your household have a telephone(fixed landline or mobile)?\",\n",
    "    \"colour TV\":\"Does your household have a color TV?\",\n",
    "    \"washing machine\":\"Does the household have a washing machine? \",\n",
    "    \"van\":\"Does your household have a car/van for private use? \",\n",
    "    \"dwelling\":\"Do you have any of the following problems with your dwelling/accommodation? \",\n",
    "    \"warm\":\"Can your household afford to keep its home adequately warm?  \"\n",
    "    } \n",
    "    translated_keywords_dict = defaultdict()\n",
    "    for key in questions_to_keywords.keys():\n",
    "        translated_keywords_dict[GoogleTranslator(source='en', target='french').translate(key)] = []\n",
    "    return translated_keywords_dict\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#iterates through an array which contains page numbers, extracts each quesiton from that page, translates them into english,\n",
    "#adds to an array, cleans data and adds to final array \n",
    "def translate_document(pages):\n",
    "    pdf1 = pdfplumber.open(\"france.pdf\")\n",
    "    translated_array = []\n",
    "    #pages = list(pages)\n",
    "    # writing page 161 will translate page 162\n",
    "    pages= pages\n",
    "    for number in pages:\n",
    "        p1 = pdf1.pages[number]\n",
    "        im = p1.to_image()\n",
    "        text = p1.extract_text()\n",
    "        text = clean(text)\n",
    "        #text = re.split('[?]',text)  \n",
    "        text = re.findall('(?<=[\\?\\.\\!]\\s)[^\\?\\n\\.]+?\\?',text)\n",
    "        clean_sent  = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent) \n",
    "        #translated_array.append(clean_sent)\n",
    "        translated_array.append(translator(clean_sent))\n",
    "    return translated_array\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    translated_keywords = keywords().keys()\n",
    "    pages = []\n",
    "    pdf = pdfplumber.open(\"france.pdf\")\n",
    "    for word in translated_keywords:\n",
    "        word = word.lower()\n",
    "        for i in range(0,len(pdf.pages)):\n",
    "            page_number = pdf.pages[i]\n",
    "            Text = page_number.extract_text()\n",
    "            if re.findall(word,Text,re.IGNORECASE):\n",
    "                pages.append(i)\n",
    "    print(pages)\n",
    "    clean_translations = flatten_list(translate_document(pages))\n",
    "    #d = {'Translated Questions':translate_document(pages)}\n",
    "    d = {'Translated Questions':clean_translations}\n",
    "    DftranslatedDoc=pd.DataFrame(data =d)\n",
    "    display(DftranslatedDoc) \n",
    "    DftranslatedDoc.to_csv('out_translation.csv',index=False) \n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "lemmatizier= WordNetLemmatizer()\n",
    "for index,row in DftranslatedDoc.iterows():\n",
    "    filttered_sentence=[]\n",
    "    sentence = row['col1']\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    words=nltk.word_tokenize(sentence)\n",
    "    words=[ w for rw in words if not w in stop_words]\n",
    "    for word in words:\n",
    "        filttered_sentence.append(lemmatizer.lemmatize(word))\n",
    "    #print(filttered_sentence)\n",
    "    data.ix[index,'col1'] = filttered_sentence\n",
    "        \n",
    "\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after finding questions on relevant pages, translates only the qusetions that contain the keywords we're looking for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 14, 18, 29, 31, 32, 41, 43, 45, 48, 55, 57, 58, 184, 188, 254, 274, 334, 336, 183, 186, 151, 185, 254, 258, 259, 261, 262, 3, 4, 12, 34, 107, 108, 109, 191, 332, 106, 107, 2, 4, 5, 6, 7, 14, 17, 18, 23, 25, 28, 30, 31, 32, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 48, 52, 55, 56, 57, 58, 63, 64, 69, 70, 71, 72, 73, 74, 77, 83, 84, 85, 86, 87, 88, 90, 91, 97, 98, 99, 105, 117, 124, 125, 126, 127, 128, 130, 131, 132, 134, 153, 157, 164, 165, 170, 173, 177, 179, 180, 185, 189, 194, 196, 197, 198, 199, 200, 201, 203, 206, 212, 218, 227, 228, 232, 241, 251, 252, 254, 255, 257, 258, 259, 260, 261, 262, 263, 270, 271, 272, 273, 274, 275, 279, 281, 283, 287, 288, 290, 291, 292, 294, 297, 298, 300, 302, 303, 307, 309, 311, 314, 321, 322, 323, 325, 326, 327, 330, 331, 337, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 70, 79, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 103, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 134, 135, 136, 137, 140, 151, 152, 153, 154, 156, 158, 159, 161, 182, 183, 193, 259, 262, 274, 299, 305, 322, 339]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Answer in clear)……………………………… Go to VALIDF If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>» if BS &gt; 0 (Y14) According to you, is the acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A leisure or holiday residence?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Occasional accommodation for studies or work?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Otherwise: TYPMEN Occupation of the same accom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Other benefits in kind (free or preferential r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>» If AVNAT=5, NSP or REFUSAL, go to FINSAL If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>If SITIND = (B), (C), (D) or (SITIND = (A) and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>No NSP REFUSAL If EXRMI=1 and if CAISSALAR =1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>To a person outside the current household DK R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Translated Questions\n",
       "0    (Answer in clear)……………………………… Go to VALIDF If ...\n",
       "1    » if BS > 0 (Y14) According to you, is the acc...\n",
       "2                      A leisure or holiday residence?\n",
       "3        Occasional accommodation for studies or work?\n",
       "4    Otherwise: TYPMEN Occupation of the same accom...\n",
       "..                                                 ...\n",
       "418  Other benefits in kind (free or preferential r...\n",
       "419  » If AVNAT=5, NSP or REFUSAL, go to FINSAL If ...\n",
       "420  If SITIND = (B), (C), (D) or (SITIND = (A) and...\n",
       "421  No NSP REFUSAL If EXRMI=1 and if CAISSALAR =1,...\n",
       "422  To a person outside the current household DK R...\n",
       "\n",
       "[423 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import PyPDF2\n",
    "import re\n",
    "import operator \n",
    "import numpy as np\n",
    "\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to test different regex expressions\n",
    "#so findall definitely works for matching aceented characters \n",
    "#problem is with the pdf extractor that we are using\n",
    "def test_regex():\n",
    "    matched = (re.findall('é','hé'))\n",
    "    print(\"matched\",matched)\n",
    "\n",
    "\n",
    "\n",
    "#translator function - given an array of lines, translate each line in the array, add to array of translated lines,\n",
    "#and return \n",
    "def translator(lines):\n",
    "    translated_array  = []\n",
    "    for i in lines:\n",
    "        to_translate = i \n",
    "        translated  = GoogleTranslator(source='auto',target='en').translate(i)\n",
    "        translated_array.append(translated)\n",
    "    return translated_array \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cleans text from any whitesace and can later be used to remove punctuation if necessary \n",
    "def clean(text):\n",
    "    text = re.sub('\\n','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #removing punctuation \n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text \n",
    "\n",
    "\n",
    "#contains the kwyrods for each poverty question and translates them into the target language - change later so that \n",
    "#you can change the language we are using \n",
    "def keywords():\n",
    "    questions_to_keywords={\n",
    "    \"holiday\":\"Can your whole household afford to go for a week’s annual holiday, away from home?\",\n",
    "    \"vegetarian\":\"Can your household afford a meal with meat, chicken, fish(or vegetarian equivalent)?\",\n",
    "    \"expense\": \"Can your household afford an unexpected required expense(amount to be filled) and pay through its own resources?\",\n",
    "    \"telephone\":\"Does your household have a telephone(fixed landline or mobile)?\",\n",
    "    \"colour TV\":\"Does your household have a color TV?\",\n",
    "    \"washing machine\":\"Does the household have a washing machine? \",\n",
    "    \"van\":\"Does your household have a car/van for private use? \",\n",
    "    \"dwelling\":\"Do you have any of the following problems with your dwelling/accommodation? \",\n",
    "    \"warm\":\"Can your household afford to keep its home adequately warm?  \"\n",
    "    } \n",
    "    translated_keywords_dict = defaultdict()\n",
    "    for key in questions_to_keywords.keys():\n",
    "        translated_keywords_dict[GoogleTranslator(source='en', target='french').translate(key)] = []\n",
    "    return translated_keywords_dict\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#iterates through an array which contains page numbers, extracts each quesiton from that page, translates them into english,\n",
    "#adds to an array, cleans data and adds to final array \n",
    "def translate_document(pages):\n",
    "    pdf1 = pdfplumber.open(\"france.pdf\")\n",
    "    translated_array = []\n",
    "    #pages = list(pages)\n",
    "    # writing page 161 will translate page 162\n",
    "    pages= pages\n",
    "    keyword_list = keywords().keys()\n",
    "    for number in pages:\n",
    "        p1 = pdf1.pages[number]\n",
    "        im = p1.to_image()\n",
    "        text = p1.extract_text()\n",
    "        text = clean(text)\n",
    "        #text = re.split('[?]',text)  \n",
    "        text = re.findall('(?<=[\\?\\.\\!]\\s)[^\\?\\n\\.]+?\\?',text)\n",
    "        for sentence in text:\n",
    "            y = any(x in sentence for x in keyword_list)\n",
    "            if not y:\n",
    "                text.remove(sentence)\n",
    "        clean_sent  = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent) \n",
    "        #translated_array.append(clean_sent)\n",
    "        translated_array.append(translator(clean_sent))\n",
    "    return translated_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    translated_keywords = keywords().keys()\n",
    "    pages = []\n",
    "    pdf = pdfplumber.open(\"france.pdf\")\n",
    "    for word in translated_keywords:\n",
    "        word = word.lower()\n",
    "        for i in range(0,len(pdf.pages)):\n",
    "            page_number = pdf.pages[i]\n",
    "            Text = page_number.extract_text()\n",
    "            if re.findall(word,Text,re.IGNORECASE):\n",
    "                pages.append(i)\n",
    "    print(pages)\n",
    "    clean_translations = flatten_list(translate_document(pages))\n",
    "    #d = {'Translated Questions':translate_document(pages)}\n",
    "    d = {'Translated Questions':clean_translations}\n",
    "    DftranslatedDoc=pd.DataFrame(data =d)\n",
    "    display(DftranslatedDoc) \n",
    "    DftranslatedDoc.to_csv('out_translation.csv',index=False) \n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
