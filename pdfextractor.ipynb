{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f010db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcfa6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lafeizhuangyuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lafeizhuangyuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import PyPDF2\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "pdf = pdfplumber.open(\"france.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e0c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vacances', 'v√©g√©tarien', 'frais', 'T√©l√©phone', 't√©l√©viseur couleur', 'Machine √† laver', 'van', 'logement', 'chaleureux']\n"
     ]
    }
   ],
   "source": [
    "def keywords():\n",
    "    #this is not the whole list of the keywords\n",
    "    questions_to_keywords={\n",
    "    \"holiday\":\"Can your whole household afford to go for a week‚Äôs annual holiday, away from home?\",\n",
    "    \"vegetarian\":\"Can your household afford a meal with meat, chicken, fish(or vegetarian equivalent)?\",\n",
    "    \"expense\": \"Can your household afford an unexpected required expense(amount to be filled) and pay through its own resources?\",\n",
    "    \"telephone\":\"Does your household have a telephone(fixed landline or mobile)?\",\n",
    "    \"colour TV\":\"Does your household have a color TV?\",\n",
    "    \"washing machine\":\"Does the household have a washing machine? \",\n",
    "    \"van\":\"Does your household have a car/van for private use? \",\n",
    "    \"dwelling\":\"Do you have any of the following problems with your dwelling/accommodation? \",\n",
    "    \"warm\":\"Can your household afford to keep its home adequately warm?  \"\n",
    "} \n",
    "    \n",
    "    translated_keywords = []\n",
    "    for key in questions_to_keywords.keys():\n",
    "        translated = GoogleTranslator(source='en', target='french').translate(key)\n",
    "        translated_keywords.append(translated)\n",
    "    return translated_keywords\n",
    "\n",
    "    \n",
    "    \n",
    "translated_keywords = keywords()\n",
    "print(translated_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf2f88",
   "metadata": {},
   "source": [
    "# Text Preprocessing functions üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc8ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing regex - re.IGNORECASE works so it should work when matching keywords but doesnt for some reason\n",
    "def test_regex():\n",
    "    matched = []\n",
    "    matched.append(re.findall('h√©',\"h√©llo\"))\n",
    "    return matched\n",
    "\n",
    "\n",
    "                               \n",
    "#function to translate from detected language into english using deep translator library \n",
    "def translator(lines):\n",
    "    translated_array =[]\n",
    "    for i in lines:\n",
    "        to_translate = i\n",
    "        translated = GoogleTranslator(source='auto', target='en').translate(i)\n",
    "        translated_array.append(translated)\n",
    "    return translated_array\n",
    "\n",
    "\n",
    "#cleans text removing next lines, whitespace,punctuation,setc \n",
    "def clean(text):\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #removing punctuations'\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text) :\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01160a07",
   "metadata": {},
   "source": [
    " **Stop words/(stemming)/ Lemmatization function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b0f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lafeizhuangyuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words= stopwords.words('english') # can be changed to ('french')\n",
    "#stop_words.append('unimportant word')\n",
    "#print (stop_words)\n",
    "\n",
    "#stemmer works not good\n",
    "#from nltk.stem import PorterStemmer#WordNetLemmatizer\n",
    "#stemmer = PorterStemmer()\n",
    "#input_str = \"I loved apples\"\n",
    "#print(stemmer.stem(input_str))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#input_str = \"I loved apples\"\n",
    "#for word in input_str:\n",
    "    #print(lemmatizer.lemmatize(word))\n",
    "#print(lemmatizer.lemmatize(input_str))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb0c3a",
   "metadata": {},
   "source": [
    "# the main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d184a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    col1\n",
       "0     []\n",
       "1     []\n",
       "2     []\n",
       "3     []\n",
       "4     []\n",
       "..   ...\n",
       "311   []\n",
       "312   []\n",
       "313   []\n",
       "314   []\n",
       "315   []\n",
       "\n",
       "[316 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DO NOT CHANGE- FULLY IDENTIFIES QUESTIONS ON A PAGE\n",
    "#given a page of a docuent or list of pages, will find all questions on that page and translate into english \n",
    "def translate_document(pages):\n",
    "    pdf1 = pdfplumber.open(\"france.pdf\")\n",
    "    translated_array = []\n",
    "    #pages = list(pages)\n",
    "    # writing page 161 will translate page 162\n",
    "    pages= pages\n",
    "    for number in pages:\n",
    "        p1 = pdf1.pages[number]\n",
    "        im = p1.to_image()\n",
    "        text = p1.extract_text()\n",
    "        text = clean(text)\n",
    "        #text = re.split('[?]',text)  \n",
    "        text = re.findall('(?<=[\\?\\.\\!]\\s)[^\\?\\n\\.]+?\\?',text)\n",
    "        clean_sent  = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent) \n",
    "        translated_array.append(clean_sent)\n",
    "        translated_array.append(translator(clean_sent))\n",
    "    return translated_array\n",
    "\n",
    "\n",
    "\n",
    "pages = []\n",
    "for word in translated_keywords:\n",
    "    word = word.lower()\n",
    "    pdf = PyPDF2.PdfFileReader(r\"france.pdf\")\n",
    "    for i in range(0,pdf.getNumPages()):\n",
    "        PageObj = pdf.getPage(i)\n",
    "        Text = PageObj.extractText()     \n",
    "        if re.findall(word,Text,re.IGNORECASE):\n",
    "            #pages.append(word)\n",
    "            pages.append(i) \n",
    "            \n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "d = {\"col1\": translate_document(pages)}\n",
    "DftranslatedDoc=pd.DataFrame(data =d)\n",
    "DftranslatedDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f21e2",
   "metadata": {},
   "source": [
    "## Preprocessing used on  our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dadf820f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3841773175.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/fn/qk82czzs25vdb0pcdmk4t3280000gn/T/ipykernel_2849/3841773175.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    filttered_sentence=[]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmatizier= WordNetLemmatizer()\n",
    "for index,row in DftranslatedDoc.iterows():\n",
    "    filttered_sentence=[]\n",
    "    sentence = row['col1']\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    words=nltk.word_tokenize(sentence)\n",
    "    words=[ w for rw in words if not w in stop_words]\n",
    "    for word in words:\n",
    "        filttered_sentence.append(lemmatizer.lemmatize(word))\n",
    "    #print(filttered_sentence)\n",
    "    data.ix[index,'col1'] = filttered_sentence\n",
    "        \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4712dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d3326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pdfextractor]",
   "language": "python",
   "name": "conda-env-pdfextractor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
